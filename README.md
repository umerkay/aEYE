![image](https://github.com/umerkay/dl_project/assets/20483712/ffdfdd77-7072-429f-ae25-2b3eeb91c5b1)

# aEYE: Visual Question Answering (VQA) for Visually Impaired Individuals

## Overview

aEYE is a Visual Question Answering (VQA) system designed to empower visually impaired individuals by providing access to visual content through natural language questions. This system allows users to inquire about their surroundings, identify objects, and understand scenes independently.

![image](https://github.com/umerkay/dl_project/assets/20483712/9d4ee519-72d9-432b-821f-757750b9dc22)


## Problem Statement

Visually impaired individuals often face challenges accessing visual information, which hinders their understanding of surroundings and objects. Traditional methods such as audio descriptions or tactile representations are limited in spontaneity and comprehensiveness. aEYE aims to bridge this gap by providing real-time visual content understanding through natural language processing.

## Objectives

- Provide an accessible VQA system tailored for blind users.
- Enable users to independently inquire about their surroundings.
- Facilitate identification of objects and understanding of scenes through natural language questions.

## Implementation Details

Our solution involves fine-tuning a pretrained version of Microsoftâ€™s GenerativeImage2Text (GIT) model on the COCOQA dataset.

![image](https://github.com/umerkay/dl_project/assets/20483712/5d521b95-f8af-4825-8945-05128b418ff1)


### Model Architecture

- **Image Encoder**: A contrastive pre-trained model that takes a raw image as input and outputs a 2D feature map.
- **Text Decoder**: A transformer module consisting of multiple transformer blocks, each with a self-attention layer and a feed-forward layer.
- **Input Text**: Tokenized and embedded, concatenated with the image features from the image encoder. The input text includes the question and the ground-truth answer as a special caption.

### Training

- **Dataset**: COCOQA
- **Epochs**: 50
- **Optimizer**: Adam
- **Loss Function**: Cross Entropy applied to the answer and EOS tokens.

### Results

- **Accuracy**: Achieved a cosine similarity accuracy of 0.72 between the predicted answer and the target answer.
- **Training and Validation Loss**:
![image](https://github.com/umerkay/dl_project/assets/20483712/5b262c67-1591-41f2-add0-926cf067686b)


## Limitations and Future Directions

- **Current Limitations**:
  - Most answers are single words due to the dataset's constraints.
  - Model complexity results in slow computation during inference, especially on a CPU.
  
- **Future Directions**:
  - Fine-tuning on a larger dataset with more comprehensive answers.
  - Optimizing the model for local performance by storing image features data while performing text prompt inference.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/umerkay/aEYE.git
   cd aEYE
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the web application:
   ```bash
   flask run
   ```

## Usage

1. Access the web application through your browser at http://localhost:5000/
2. Upload an image and ask a question about it using natural language.
3. Receive the answer generated by the model.

## References

- GenerativeImage2Text (GIT): [Paper](https://arxiv.org/abs/2205.14100)
